%!TEX root = main.tex

%For each of the websites the module had to classify, information about the volume of consumed data on each of the connections was also available. 
%Figure \ref{fig:{img/volume_stacked.png}} is the result of combining the classification results with that information, before applying the language classification.
%the other data,

\section{Results}

As we saw in previous chapters, FlowSight was already able to graphically represent the amount of consumed data in time \ref{fig:{img/total_traffic_browsing.png}}. The objective of this thesis was to
give FlowSight the power to accurately and efficiently categorize the traffic depending on the content of the visited URL. As we can see in figures \ref{fig:{img/volume_stacked.png}} and 
\ref{fig:{img/volume_plain.png}} the objective was achieved.
 
\clearpage
\imatge{img/total_traffic_browsing.png}{Total browsing traffic}
\imatge{img/volume_stacked.png}{Stacked representation of the volume of different categories}
\imatge{img/volume_plain.png}{Plain representation of the volume of different categories}

%!TEX root = main.tex



\section{Enhancements}

There are several difficulties in the classification task that can be optimized performing some refination. 
The first difficulty is related to those categories which are more difficult to distinguish for some reason. Another difficulty is the concept of \codi{Other} category, which is not trivial to
model in a machine learning area.
Other difficulties are that some servers returned their web content in the language of the country they detected the request was coming from. Changing the header in the requests and using a proxy 
solved this problem.  
Another difficulty in this classification task is the fact that the content of some of the web pages that are being classified is written in languages different from english. 
As in this specific problem the classifier is trained to work in English, this situation will potentially cause wrong classifications. 
We could just filter those web pages from foreign domains (\codi{.cz, .jp, .cn}) and automatically discard them, but some of those webpages could be written in english, and we would still have the same 
problem with the generic \codi{.com} domain.
A better approach would be identifying the language itself and discarding the instances  which are not in english.


\subsection{Ambiguous categories}


\subsubsection{Entertainment into subcategories}
%!Splitting entertainment into subcategories and seeing performance
As we saw in Chapter \ref{chap:research}, the category \codi{Entertainment} is one of the most difficult to distinguish. This could be due to the fact that it is a wide category containing a lot of different
subcategories. We could try to split \codi{Entertainment} into subcategories (such as \codi{Music}, \codi{Movies}, \codi{Literature}...) to get a more precise dataset. In order to meet the
client requirements those subcategories should be remapped to the original \codi{Entertainment} category after the classification process. \\
In Figure \ref{fig:{graphs/confusion1}} we can observe the mean accuracy for each of the categories together with the category that would have been chosen if the first one didn't existed. New
categories are added to check their relevance. We can see how the entertainment category is being a common second option for almost all categories. We can also observe that the confidence of the 
\codi{Food \& Drink} category is very high.
As new classes are added, it is clear that the overall accuracy of the classification may be worse, but with this new structure the confusions are likely to be between categories belonging to the same
supercategory, which is not a problem.
\grafic{graphs/confusion1}{Categorization confusions before split}

After splitting the \codi{Entertainment} category into subcategories, these were the new classes representing it: \\
\begin{lstlisting} 
Music, Literature, Theme parks, Movies, Food & Drink, Games, Television
\end{lstlisting} 
Figure \ref{fig:{graphs/confusion2}} shows the same information as 
Figure \ref{fig:{graphs/confusion1}} but without the generic \codi{Entertainment} category. Now we see how the \codi{News} category is playing the role that \codi{Entertainment} was previously playing.

\grafic{graphs/confusion2}{Categorization confusions after split}

%TODO Somehow test which approach is better!
 

\subsubsection{The \codi{Other} category}
There are several approaches in order to represent the \codi{Other} category, one of them could be creating a new class named \codi{Other} and filling it with content from categories that 
differ from the ones we want to classify.\\
Another approach is taking the accuracy of a classification into account, and using a threshold to decide whether some sample should be considered of a any class or assume it should be considered of 
another class. 
By calculating the variance of the probability distribution through classes, we can determine the strength with which a classification is done.   

\grafic{graphs/other}{Probability distribution of two instances}

Figure \ref{fig:{graphs/other}} represents the confidence with which two instances are classified, the x axis being the classes
and the y axis being the probability. We could say that the blue colored classification is not really confident whereas the orange colored classification seems to be sure assigning the \codi{c30} 
category. In this particular case we could assign the blue instance to the \codi{Other} category, because it is more or less equally unsure of where to assign it.

%TODO Somehow test which approach is better!

\subsubsection{Testing}
Taking into account the previous two sections, I performed an experiment using different permutations in the training data set, as well as implementations of the \codi{Other} category. 
The second column in table \ref{tab:accuracy} represents whether the \codi{Entertainment} category is split into subcategories or not. The third column represents whether the \codi{Other} category
is split into subcategories or not, or if it is actually used to train the model or the variance approach is used. The last column represents the accuracy of the model training. \\
 
\begin{table}[htbp]
\caption{Accuracy evaluation using different training data structure}
\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline
\textbf{id} & \textbf{Entertainment} & \textbf{Other} & \textbf{Accuracy} \\ \hline
1 & Split & Split & 77.33\% \\ \hline
2 & Split & No data & 77.40\% \\ \hline
3 & Not split & Not split & 65.07\% \\ \hline
4 & Not split & Not split & 79.19\% \\ \hline
5 & Not split & No data & 78.23\% \\ \hline
\end{tabular}
\end{center}
\label{tab:accuracy}
\end{table}

As we can see trainings 3 and 4 are identically set but give different accuracy percentages. This is due to how the construction of the training directories was made. In example 3, the structure
of example 1 was used as a base and the \codi{Entertainment} and \codi{Other} directories were constructed by merging all related directories, having as a result, two categories with almost 5 times more data
than the others. The low accuracy percentage gives us a hint of what may have happened. Having too many data of two classes might have caused the model to overfit the data. In example 4, which has
a balanced number of examples gives a much better result. 

\clearpage
\grafic{graphs/distrib.tex}{Example distribution using the different combinations}


\subsection{Language Identification}
As we saw in the introduction of this Chapter, in some situations web page content might be in a language different from English and as the model is trained in English this would lead to wrong 
classifications. Detecting the web page content language and separating those URL from the rest would give a more accurate result.
Language identification is the process of determining which natural language given content is in. Using it, we could avoid the classification of foreign websites and enhance the overall accuracy.
There are several computational/statistical approaches to solve this problem which is split into two different stages: modeling and classification

\subsubsection{Modeling techniques}
Before being able to classify languages we need to generate models that will represent each language. This can be done using different techniques:
\begin{itemize}
  \item {\bf Common words technique}: the most common words in a corpus are sorted by frequency in order to get a probability distribution.
  \item {\bf N-Gram technique}: similar to common words technique but instead of sorting the most common words, the most common successions of N characters are sorted.
\end{itemize}  
The disadvantage of the {\bf common words technique} is that although common words may occur in large amounts of text, they might not occur in shorter input examples. With the {\bf N-Gram} technique 
we avoid that disadvantage by not using just the words but also all the N-partitions in the text.


\subsubsection{Classification techniques}
Once we have a model for each language we want to identify, we have to classify them. There are different approaches for this task too:
\begin{itemize}
  \item {\bf Relative entropy}: compares the compressibility of the text we want to identify to the compressibility of the previously generated models.
  \item {\bf Rank order statistics}: generates a probability by comparing how far an N-Gram is from the position of the same N-Gram in the model.
\end{itemize}
\subsubsection{Application}
For the scope of this project I chose the {\bf 3-Gram technique} to generate the language models and the {\bf Rank order statistics} to identify the languages. I used a Python implementation by Damir 
Cavar\cite{lid1} consisting of a model generator and a classifier.
For generating the language models, I used the Wikipedia as a text source by automatically accessing random articles in different languages and crawling their content. 
After generating the models a simple Python algorithm remapped the classification file taking into account the language information.

\begin{lstlisting}[language=Python]

import lid
import os
import csv

myLid = lid.Lid()
output_file = "language_categorized.csv"
input_file = "categorized.csv"

seed_write = csv.writer(open(output_file, 'wb'), delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)
seed_read = csv.reader(open(input_file, 'rU'), delimiter=',')

for row in seed_read:
  try:
    # This call returns the language of the file content
    language = myLid.checkText(open("unknown-data/http:"+row[0]+".out").read())
    if (language != "English"):
      seed_write.writerow([row[0],"foreign_language"])
    else:
      seed_write.writerow([row[0],row[1]])
  except Exception, e:
    print "error identifying language of "+row[0]
    print e
    seed_write.writerow([row[0],"other"])
\end{lstlisting}

Before applying the LID algorithm, websites in other languages were being categorized as random categories, adding unuseful noise to the results. After applying the algorithm, those websites were 
categorized as \codi{foreign\_language} and were easily identified. We can see in Figure \ref{fig:{graphs/lid1}} how almost 5000 (3\%) foreign language web pages were wrongly classified and also which
categories where most affected by this noise.

\grafic{graphs/lid1}{Applying the LID algorithm}

\begin{lstlisting}
...
safir.is,foreign_language
trafikrapport.com,foreign_language
klc.fi,foreign_language
letemsvetemapplem.eu,foreign_language
mediaindex.ee,foreign_language
foceni.org,foreign_language
ye.dk,foreign_language
oddschecker.es,foreign_language
m-app.jp,foreign_language
synergysuite.net,foreign_language
mediamatic.nl,foreign_language
smachno.ua,foreign_language
justrewardsuk.co.uk,foreign_language
webnode.cz,foreign_language
...
\end{lstlisting}
