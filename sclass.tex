%!TEX root = main.tex

The main goal of the project was to be able to automatically classify webpages depending on their content into different categories. 
By the goal itself it is clear that Machine Learning algorithms might be the best approach to try to solve the problem. 

\subsection{Client requirements}

The original set of categories that we were asked to identify were the following.

\codi{Social Networking, Email, Sport, News, Shopping, Weather, Travel, Maps, Photos, Instant Messaging, Entertainment, Financial, Adult, Other}

Before going any further and just looking at the categories we were asked to classify, we can observe that some of them could be more difficult to classify.
Some categories like \codi{Adult} had a clearly characteristic language in their content whereas other categories like \codi{Email} might contain generic language.
We can also see that the category \codi{Entertainment} might be overlapping with others like \codi{Photos}, \codi{Travel}, \codi{Social Networking}, etc. 
Finally one of the difficulties of the task was to categorize something as \codi{Other}. 
For initial testing purposes that could help taking later decisions I used rapidminer. A fast-to-set and easy-to-use machine learning tool that, among other things, allowed me to test different
machine learning algorithms with the same datasets in order to compare their performance.

\subsection{Crawling training data}
Supervised Machine Learning algorithms need a set of training (labeled) data to infere the function that will later determine wether a new example is from one class or the other. In this case we needed
web content for each of the categories. We decided to use the {\bf Yahoo! Directory}\cite{yahoo} as a source for this data.
The {\bf Yahoo! Directory} is an organized url database where everything is under a category. Crawling every url under a given category provided a huge reliable corpus to train the model.

  


\subsection{Tests with Rapidminer}
Rapidminer\cite{rapidminer} is a widely used easy to use and powerful machine learning environment.  
It was the perfect tool to do some initial tests in orther to diagnose how might some categories be overlapping, detecting which algorithms performed better, etc.
\imatgePetita{img/rapid1.png}{Rapidminer process}

\subsubsection{Feature vector creation}
In order to obtain a precise and accurate document representation, and before selecting the features, several preprocesses need to/can be applied to the word set.
\imatge{img/vector.png}{Vector creation}
\begin{itemize}
  \item {\bf Tokenize}: the first step is to tokenize all documents. That is, split the text into a sequence of tokens. 
  \item {\bf Tranform cases}: transforms all words to lowercase.
  \item {\bf Stopword filter}: this process will remove common words. This step is language dependant.
  \item {\bf Stem}: this process reduces inflected or derived words to their stem, base or root. It is also language dependant.
\end{itemize}



\subsubsection{Number of features}
%!Make some experiments. Try classification using 100, 300, 500, 700, 900, max
The first experiment consisted in determining wether the amount of features was important and which was the optimal amount. Starting with 100 files for each category, the following graphic
represents the model accuracy over number of documents. 

\subsubsection{K nearest neighbours}
%!Do some tests changing the N and see the performance.

\subsubsection{X-Accuracy}
%!Learn to distinguish between all pairs of classes and see which are more difficult
Another experiment consisted in comparing all classes binarily to  


\subsubsection{Entertainment into subcategories}
%!Splitting entertainment into subcategories and seeing performance
As we saw in the previous section, the category \codi{Entertainment} was one of the most difficult to distinguish. 

\subsubsection{The \codi{Other} category}

\subsection{MaxEnt implementation in the Framework}


\subsubsection{Feature selection}
Word count

\begin{itemize}
\item Compute $d_j, j=1,...,k+1$
\item Initialize $\lambda_j^{(1)}$ to zero
\item Repeat until converge
\item For each j 
  \begin{itemize}
  \item Compute $E_{p^{(n)}} f_j = \sum\limits_{x \in \varepsilon} p^{(n)} (x)f_j(x)$
    where $p^{(n)}(x) = \frac{e^{\sum\limits_{j=1}^{k+1}\lambda_j^{(n)}f_j(x)}}{Z}$ 
  \item Update $\lambda_j^{(n+1)} = \lambda_j^{(n)} + \frac{1}{C}(log\frac{d_i}{E_{{p^{(n)}}}f_j})$
  \end{itemize}
\end{itemize}

  
Looking at the rapidminer test results, we could say that Naive Bayes was the better performing algorithm. 

Why maximum entropy?
Maximize ntropy = Minimize commitment

Model all that is known and assume nothing about what is unknown. 
Model all that is known: satisfy a set of constraints that must hold respecting uniformity.

Assume nothing about what is unknown: 
   choose the most “uniform” distribution 
   choose the one with maximum entropy

One nice aspect of maximum entropy is that it does not suffer from any independence assumptions. For example, consider a document containing the same number of occurences of
"rainy" and "social network". A Naive Bayes classifier will double count the occurences of "social network" - it will add in the weight for "social" and the weight for "network". Since "rainy" and
"social network" occur equally, a single occurrence of "social network" will contribute twice the weight as an occurence of "rainy". This will cause Naive Bayes to prefer one class incorrectly.
Maximum entropy, on the other hand, will discount the $\lambda_i$ for each of these features such that their weight towards classification is appropriately reduced by half.
This is because the constraints work over expectations of the counts. One implication for this freedmo from independence assumptions is that bigrams and phrases can be easily added as features 
by maximum entropy, without worry that the features are overlapping.i


\subsubsection{Performace}
%!Run the tests with m1r1, m2r1, m3r1, m4r1, m5,r1, mr2r2, m3r3, m4r4, m5r5, or etc...
Compared to other classification algorithms like Naive Bayes, the Maximum Entropy algorithm has a higher computational requirement. 
The platform where the MaxEnt classification algorithm is running is highly scalable and follows the map/reduce paradigm. Taking advantage of the paraidgm, the MaxEnt algorithm
is able to perform notably faster.
The following chart presents the execution time of the same algorithm with different number of mappers and reducers.


\setlength{\unitlength}{0.3cm}
\begin{picture}(15,15)(-20,2)
\thicklines

% Axis
\put(0,0){\line(0,1){15}}
\put(0,0){\line(1,0){15}}
\put(0,0){\line(-1,-1){7}}

% Y-axis points (performance)
\put(0,1){\circle*{0.25}}
\put(0,2){\circle*{0.25}}
\put(0,3){\circle*{0.25}}
\put(0,4){\circle*{0.25}}
\put(0,5){\circle*{0.25}}
\put(0,6){\circle*{0.25}}
\put(0,7){\circle*{0.25}}
\put(0,8){\circle*{0.25}}
\put(0,9){\circle*{0.25}}
\put(0,10){\circle*{0.25}}
\put(0,11){\circle*{0.25}}
\put(0,12){\circle*{0.25}}
\put(0,13){\circle*{0.25}}
\put(0,14){\circle*{0.25}}
\put(0,15){\circle*{0.25}}


% X-axis points (mappers)
\put(3,0){\circle*{0.25}}
\put(6,0){\circle*{0.25}}
\put(9,0){\circle*{0.25}}
\put(12,0){\circle*{0.25}}
\put(15,0){\circle*{0.25}}

% Z-axis points (reducers)

\put(-1.41421356237,-1.41421356237){\circle*{0.25}}
\put(-2.82842712475,-2.82842712475){\circle*{0.25}}
\put(-4.24264068712,-4.24264068712){\circle*{0.25}}
\put(-5.65685424949,-5.65685424949){\circle*{0.25}}
\put(-7.07106781187,-7.07106781187){\circle*{0.25}}

\thinlines
% Actual data

\put(1.58578643763,1.02967905463){\circle{0.4}}
\put(0.171572875254,6.78483100295){\circle{0.4}}
\put(-1.24264068712,9.62052038268){\circle{0.4}}
\put(-2.65685424949,2.32855143761){\circle{0.4}}
\put(-4.07106781187,-2.84521281827){\circle{0.4}}
\put(4.58578643763,1.15627967123){\circle{0.4}}
\put(3.17157287525,6.80336543285){\circle{0.4}}
\put(1.75735931288,9.59186656708){\circle{0.4}}
\put(0.343145750508,1.85443578311){\circle{0.4}}
\put(-1.07106781187,-3.66517960337){\circle{0.4}}
\put(7.58578643763,1.15295345123){\circle{0.4}}
\put(6.17157287525,6.80235629055){\circle{0.4}}
\put(4.75735931288,9.80411970408){\circle{0.4}}
\put(3.34314575051,0.567089272608){\circle{0.4}}
\put(1.92893218813,-7.07106781187){\circle{0.4}}
\put(10.5857864376,1.13855209883){\circle{0.4}}
\put(9.17157287525,6.80154912895){\circle{0.4}}
\put(7.75735931288,9.72005336158){\circle{0.4}}
\put(6.34314575051,0.0693443842076){\circle{0.4}}
\put(4.92893218813,-0.878358846965){\circle{0.4}}
\put(13.5857864376,1.12938781753){\circle{0.4}}
\put(12.1715728753,6.81717827025){\circle{0.4}}
\put(10.7573593129,9.74711174798){\circle{0.4}}
\put(9.34314575051,0.681474333708){\circle{0.4}}
\put(7.92893218813,-0.280308976565){\circle{0.4}}

%\put(0.7,0.3){$A$}
%\put(4.05,1.9){$B$}
%\put(1.7,2.95){$C$}
%\put(3.1,2.5){$a$}
%\put(1.3,1.7){$b$}
%\put(2.5,1.05){$c$}
%\put(0.3,4){$F=
%\sqrt{s(s-a)(s-b)(s-c)}$}
%\put(3.5,0.4){$\displaystyle
%s:=\frac{a+b+c}{2}$}
\end{picture}
