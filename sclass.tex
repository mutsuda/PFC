%!TEX root = main.tex

The main goal of the project was to be able to automatically classify webpages into different categories depending on their content. 
By the goal itself it is clear that Machine Learning algorithms might be the best approach to try to solve the problem. We could have tried to use some unsupervised algorithms to solve the task,
but in this case we have a clear knowledge about what we want to classify, and we don't need to extract new knowledge from the data.

\subsection{Client requirements}

Being part of a real case scenario brought to this project some requirements we had to take into account. The original set of categories that we were asked to identify was the following.


\begin{lstlisting}
Social Networking, Email, Sport, News, Shopping, Weather, Travel, Maps, Photos, Instant Messaging, Entertainment, Financial, Adult, Other
\end{lstlisting}

Before going any further and just looking at the categories we were asked to classify, we can observe that some of them could be more difficult to classify than others.
Some categories like \codi{Adult} have a clearly characteristic language in their content whereas other categories like \codi{Email} might contain generic language.
We can also see that the category \codi{Entertainment} might be overlapping with others like \codi{Photos}, \codi{Travel}, \codi{Social Networking}, etc. 
Another of the difficulties of the task seems to be categorizing something as \codi{Other}, because generating a training set for such a category might be difficult. 
But before being able to do any training, we needed labeled data, that is content from where to learn.

\subsection{Crawling training data}
Supervised Machine Learning algorithms need a set of training (labeled) data to infer the function that will later determine whether a new example is from one class or another. In this case we needed
web content for each of the categories. There were different approaches to solve the problem. One of them could have been querying categories in search engines or navigating through blog entries matching
each category. \\
We finally decided to use the {\bf Yahoo! Directory}\cite{yahoo} as a source for this data.
Before search engines like Google existed, people used directories to find what they were looking for. These directories are characterized as being a hierarchical structure for navigating through categories
and subcategories. Under each category there is a list of URL related to it.  
Crawling every URL under a given category provided a huge and reliable corpus to train the model.
I configured a workflow inside the application in order to automate the process of crawling the Yahoo directory, manually checking which categories needed to be crawled, and writing those initial URL into 
a seed file that was later automatically parsed.

\inputcfg{config/seed_yahoodir.txt}{Yahoo Directory! parsing seed file (excerpt)}

The process of parsing the seed file and crawling those URL was performed by two separate  workflows in the application specifically designed for the task. \\
The first workflow\ref{fig:{config/yahoo_parser.cfg}} simply parsed each of the lines inside the seed file looking for the URL contained in each of the categories and inserting them into a database. \\
A second and highly scalable workflow\ref{fig:{config/yahoo_crawler.cfg}} parsed the database crawling the content of each of the previously inserted URL and put it into a folder depending on its category.\ref{fig:{config/resultdir.txt}} \\

\inputcfg{config/yahoo_parser.cfg}{Yahoo Directory! URL parser}

\clearpage
\inputcfg{config/yahoo_crawler.cfg}{Yahoo Directory! URL crawler}

\clearpage
\inputcfg{config/resultdir.txt}{Directory structure created by the crawler task}

Once the training set was ready I performed some initial tests that could help taking later decisions using rapidminer\cite{rapidminer}, a fast-to-setup and easy-to-use machine learning tool that, 
among other things, allowed me to test different machine learning algorithms with the same datasets in order to compare their performance. 

  

\subsection{Rapidminer tests}
Rapidminer\cite{rapidminer}\ref{fig:{img/rapid1.png}} is a widely used, easy to use and powerful machine learning environment.  
It was the perfect tool to do some initial tests in order to diagnose how might some categories be overlapping, detecting which algorithms performed better, etc. Almost all training processes were 
performed with a 10 fold cross validation in order to prevent overfitting and obtaining realistic accuracy values.
\imatgePetita{img/rapid1.png}{Rapidminer process}

\subsubsection{Feature vector creation}
The fist step for all tests was reading the previously created directories and somehow turning their content into features. In rapidminer this can be done by using the \codi{Process Documents from Files}
 (see image \ref{fig:{img/read_docs.png}})
component and selecting where to read the files from.\\
\imatge{img/read_docs.png}{Read documents from files}
In order to obtain a precise and accurate document representation, and before selecting the features, several pre-processes need to/can be applied to the word set. (see image \ref{fig:{img/vector.png}})
\imatge{img/vector.png}{Vector creation} 
\begin{itemize}
  \item {\bf Tokenize}: the first step is to tokenize all documents. That is, split the text into a sequence of tokens. 
  \item {\bf Transform cases}: transforms all words to lowercase.
  \item {\bf Stopword filter}: this process will remove common words. This step is language dependant.
  \item {\bf Stem}: this process reduces inflected or derived words to their stem, base or root. It is also language dependant.
\end{itemize}


\subsubsection{Training set size}
%!Make some experiments. Try classification using 10,20,30,40,50,60,70,80,90,100,110,120,130,140,150,160,170,180,190,200,210,220,230,240,250,260...300, 500, 700, 900, max
%!Calculate time of both algorithms at 100, 200, 300, 400, 500, 600, 700, 800, 900 and 1000.
The first experiment consisted in determining how much the size of the training set mattered in terms of accuracy. It was also useful to determine which vector creation algorithm was
more suitable for the process of feature selection. Starting with 10 files for each category, the following plot represents the model accuracy over number of documents in the training set, 
using two different feature selection methods: \codi{TD-IDF} and \codi{Term Frequency}.

\grafic{graphs/features.tex}{Accuracy on number of features}
 
The conclusion of this initial test was, not surprisingly, that the more data the better. But we can also observe that when the training set size is small it is not clear which feature selection 
method is better, whereas as the dataset grows it is clear that word frequency performs better, and also faster. (see image \ref{fig:{graphs/features.tex}})

\subsubsection{K nearest neighbours}
%!Do some tests changing the N and see the performance.

\subsubsection{Face to face accuracy}
%!Learn to distinguish between all pairs of classes and see which are more difficult
Another experiment consisted in comparing all classes binarily to determine which classes are more likely to be confused. In image \ref{fig:{graphs/xclass.tex}} each circle represents a classification 
between the class in the y-axis and the class in the x-axis. Smaller and more transparent circles represent worse accuracy, and bigger and stronger circles represent better accuracy. 
\grafic{graphs/xclass.tex}{Accuracy heat graph}

As we can see, classes like \codi{Entertainment} are more likely to be confused with any other class, being the distinction between the classes \codi{News} and \codi{Entertainment} the most
difficult to perform. The reason why the \codi{News} are so easy to confuse might be the fact that their content depends on what is trending, and their sites do not contain specific language.
\codi{Entertainment} is likely to be confused with \codi{Sport} or \codi{Shopping} because a lot of people consider them entertainment activities. 
As a concrete example, the following tree represents the WJ-48 pruned tree output that classifies \codi{Weather} and \codi{News}.

\begin{lstlisting}[language=Python]
weather <= 0.068359
|   forecast <= 0.030773
|   |   snow <= 0.035962: news (511.0/30.0)
|   |   snow > 0.035962
|   |   |   stori <= 0.033615
|   |   |   |   watch <= 0.01856: weather (14.0/1.0)
|   |   |   |   watch > 0.01856: news (3.0)
|   |   |   stori > 0.033615: news (8.0)
|   forecast > 0.030773
|   |   engin <= 0.017747
|   |   |   edit <= 0.015559: weather (19.0/1.0)
|   |   |   edit > 0.015559: news (2.0)
|   |   engin > 0.017747: news (4.0)
weather > 0.068359
|   news <= 0.153393: weather (163.0/17.0)
|   news > 0.153393
|   |   copi <= 0.068359: weather (4.0)
|   |   copi > 0.068359: news (7.0)
\end{lstlisting}
It's obvious that words like weather, news or forecast will help us distinguish between one category and the other, but it is interesting to observe how the stem "stori" or "engin" also help performing
the task.


\subsubsection{Entertainment into subcategories}
%!Splitting entertainment into subcategories and seeing performance
As we saw in the previous section, the category \codi{Entertainment} is one of the most difficult to distinguish. This could be due to the fact that it is a wide category containing a lot of different
subcategories. We could try to split \codi{Entertainment} into subcategories (such as \codi{Music}, \codi{Movies}, \codi{Literature}...) to get a more precise dataset. In order to meet the
client requirements those subcategories should be remapped to the original \codi{Entertainment} category after the classification process. \\
After splitting the \codi{Entertainment} category into subcategories, these were the new classes I found: \\
\begin{lstlisting} 
Music, Literature, Theme parks, Movies, Food & Drink, Games, Television
\end{lstlisting} 
As new classes are added, it is clear that the overall accuracy of the classification may be worse, but with this new structure the confusions are likely to be between categories belonging to the same
supercategory, which is not a problem.
 

\subsubsection{The \codi{Other} category}
There are several approaches in order to represent the \codi{Other} category, one of them could be creating a new class named \codi{Other} and filling it with content from categories that 
differ from the ones we want to classify.\\
Another approach is taking the accuracy of a classification into account, and using a threshold to decide whether some sample should be considered of a any class or assume it should be considered of 
another class. 
By calculating the variance of the probability distribution through classes, we can determine the strength with which a classification is done.   

\grafic{graphs/other.tex}{Probability distribution of two instances}


This could be done by looking at the variance of the model. The plot (figure \ref{fig:{graphs/other.tex}}) represents the confidence with which two instances are classified, the x axis being the classes
and the y axis being the probability. We could say that the blue colored classification is not really confident whereas the orange colored classification seems to be sure assigning the \codi{c30} 
category. In this particular case we could assign the blue instance to the \codi{Other} category, because it is more or less equally unsure of where to assign it.


\subsection{Implementation in the Framework}
Although decision trees and Naive Bayes performed well during the rapidminer tests, they have some issues inherent to their specification. Decision trees are accurate, but their performance is 
not suitable for an almost real time high demanding environment. Naive Bayes is better at performance but it makes independence assumptions that could cause some classification problems.\\
There is an algorithm though that is both efficient (although it's not as fast as Naive Bayes) and doesn't make any assumption. It's called Maximum Entropy (or Logistic Regression).

\subsubsection{Maximum entropy concept}
Maximum entropy, also known as logistic regression focuses on minimizing commitment in its classifications. It is based on the principle of modeling all that is known satisfying a set of constraints
that must hold respecting uniformity, and assuming nothing about what is unknown.

One nice aspect of maximum entropy is that it does not suffer from any independence assumptions.\\
For example, consider a document containing the same number of occurrences of \codi{rainy} and \codi{social network}.\\
A Naive Bayes classifier will double count the occurrences of \codi{social network} 
- it will add in the weight for \codi{social} and the weight for \codi{network}. Since \codi{rainy} and
\codi{social network} occur equally, a single occurrence of \codi{social network} will contribute twice the weight as an occurrence of \codi{rainy}. This will cause Naive Bayes to prefer one class 
incorrectly.
Maximum entropy, on the other hand, will discount the $\lambda_i$ for each of these features such that their weight towards classification is appropriately reduced by half.
This is because the constraints work over expectations of the counts. \\
One implication for this freedom from independence assumptions is that bigrams and phrases can be easily added as features by maximum entropy, without worry that the features are overlapping.


\subsubsection{MaxEnt Algorithm}


\begin{itemize}
\item Compute $d_j, j=1,...,k+1$
\item Initialize $\lambda_j^{(1)}$ to zero
\item Repeat until converge
\item For each j 
  \begin{itemize}
  \item Compute $E_{p^{(n)}} f_j = \sum\limits_{x \in \varepsilon} p^{(n)} (x)f_j(x)$
    where $p^{(n)}(x) = \frac{e^{\sum\limits_{j=1}^{k+1}\lambda_j^{(n)}f_j(x)}}{Z}$ 
  \item Update $\lambda_j^{(n+1)} = \lambda_j^{(n)} + \frac{1}{C}(log\frac{d_i}{E_{{p^{(n)}}}f_j})$
  \end{itemize}
\end{itemize}

  

\subsubsection{Feature selection}
After observing the feature selection performance in the previous chapter, maybe the best choice for a fast vector creation is counting the term frequency. It usually performed better, and it always
was faster.


\subsubsection{Performance evaluation}
%!Run the tests with m1r1, m2r1, m3r1, m4r1, m5,r1, mr2r2, m3r3, m4r4, m5r5, or etc...
Compared to other classification algorithms like Naive Bayes, the Maximum Entropy algorithm has a higher computational requirement. 
The platform where the MaxEnt classification algorithm is running is highly scalable and follows the map/reduce paradigm. Taking advantage of the paradigm, the MaxEnt algorithm
is able to perform notably faster.


\grafic{graphs/sclass2dmr.tex}{2D Performance graph}

Figure \ref{fig:{graphs/sclass2dmr.tex}} represents the execution time of the same algorithm with different number of mappers and reducers, in two different machines: \codi{ultraman (Intel Core 2 Duo CPU 2.00GHz 4GB RAM)} 
is the machine where I'm writing most of this document, and \codi{rogue (Intel Core 2 Quad CPU Q9300 @ 2.5GHz 8GB RAM)} is a more powerful machine used for testing purposes. The \codi{x} axis represents the 
different mapper and reducer combinations I tested while the \codi{y} axis represents the number of categorized pages per second.
As we can see both machines perform similary when using one to two mappers, but the most powerful machine outperforms the other one when using more than two mappers. We can also observe how \codi{ultraman}'s performance
quickly stalls for more than thre mappers, while \codi{rogue} is able to hold a rising performance until six mappers are used.

Figure \ref{fig:{graphs/sclass3dmr.tex}} is a three dimensional representation of \codi{ultraman}'s performance. We can quiclky observe a repeating pattern for each number of mappers.
\grafic{graphs/sclass3dmr.tex}{3D Performance graph}

